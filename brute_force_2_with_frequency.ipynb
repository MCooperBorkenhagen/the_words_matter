{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 15:34:10.221183: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from src.learner import *\n",
    "\n",
    "samples = np.genfromtxt('data/samples.csv', delimiter=\",\").astype(bool)\n",
    "holdouts = np.genfromtxt('data/holdouts.csv', delimiter=\",\").astype(bool)\n",
    "tests = np.genfromtxt('data/tests.csv', delimiter=\",\").astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(x, K):\n",
    "    return K*math.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final hyperparameters we settled on were:\n",
    "\n",
    "Hidden units: 100  \n",
    "Learning rate: 0.01  \n",
    "Batch size: 16  \n",
    "Epochs: 50  \n",
    "\n",
    "These produced a training and testing accuracy of 0.997 and 0.986, respectively, and were associated with a runtime of under one second. While slightly higher train and test accuracies could have been achieved with more advanced tuning and training (e.g. training on more epochs) this configuration allowed us to achieve near-perfect performance on the training and holdout sets and be able to train within reasonable time horizons.  \n",
    "\n",
    "100 hidden units is the starting point. The other brute force runs (same words each iteration), we will decrease the number of hidden units by 20 succesively: 100, 80, 60, 40, 20.\n",
    "\n",
    "This procedure here differs from `brute_force_1` because we write out the test data for every word at the end of training. Not just the aggregate test data for the train and test sets respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/params.json', 'r') as f:\n",
    "    cfg = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt('data/kidwords/orth-kid.csv', delimiter=\",\")\n",
    "Y = np.genfromtxt('data/kidwords/phon-kid.csv', delimiter=\",\")\n",
    "\n",
    "words = pd.read_csv('data/kidwords/kidwords.csv', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2869, 260)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain frequencies for the frequency-weighting operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elp = pd.read_csv('~/research/words/elp/elp_full_5.27.16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = {}\n",
    "\n",
    "for word in words:\n",
    "    rowmatch = elp[elp['Word']==word]\n",
    "    if not rowmatch.empty:\n",
    "        frequencies[word] = rowmatch['Freq_HAL'].values[0]+1\n",
    "    else:\n",
    "        frequencies[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies_ = [frequencies[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = np.array([scale(frequency, cfg[\"K\"]) for frequency in frequencies_])\n",
    "\n",
    "plt.hist(weights)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write frequencies and probabilities to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5470039521000558,\n",
       " 0.4167365938096597,\n",
       " 0.691548319505963,\n",
       " 0.6598643840933528,\n",
       " 0.7196905881627325,\n",
       " 0.6867617792019002,\n",
       " 0.619162225732002,\n",
       " 0.6075526714980031,\n",
       " 0.4551201322029444,\n",
       " 0.6182388115073252,\n",
       " 0.5547716621689471,\n",
       " 0.698731488181241,\n",
       " 0.4473125619092643,\n",
       " 0.0,\n",
       " 0.886789964095843,\n",
       " 0.8167762814473631,\n",
       " 0.8822002936889397,\n",
       " 1.0006074498292192,\n",
       " 0.0,\n",
       " 0.49007021654003124,\n",
       " 0.510348086174708,\n",
       " 0.5082419525098062,\n",
       " 0.921591854965116,\n",
       " 0.49262391652792575,\n",
       " 0.6134905620221385,\n",
       " 0.5845317402501066,\n",
       " 0.6425894734449064,\n",
       " 0.682629380702156,\n",
       " 0.6198840579428377,\n",
       " 0.9140696884518528,\n",
       " 0.5060959192465104,\n",
       " 0.7317990933362715,\n",
       " 0.7129890740077088,\n",
       " 0.599952165584191,\n",
       " 0.6369334892134217,\n",
       " 0.8961619302509529,\n",
       " 0.54796343146607,\n",
       " 0.49165416887621344,\n",
       " 0.37483141241592766,\n",
       " 0.4977310726199445,\n",
       " 0.496357576001126,\n",
       " 0.41680953158512435,\n",
       " 0.0,\n",
       " 0.498201745636589,\n",
       " 0.7962851220760072,\n",
       " 0.5462452602583684,\n",
       " 0.73819080798828,\n",
       " 0.30367457061213954,\n",
       " 0.471072037184355,\n",
       " 0.600019932546068,\n",
       " 0.535726691879013,\n",
       " 0.5170212289748038,\n",
       " 0.0,\n",
       " 0.4843544312747675,\n",
       " 0.46267915062371245,\n",
       " 0.48986933793111115,\n",
       " 0.39093871582034195,\n",
       " 0.6526705882508689,\n",
       " 0.586935331422611,\n",
       " 0.0,\n",
       " 0.6677998043350996,\n",
       " 0.6033704018582269,\n",
       " 0.5454043531998006,\n",
       " 0.39332903940515995,\n",
       " 0.6542618058785827,\n",
       " 0.6403829088826898,\n",
       " 0.5592525163608097,\n",
       " 0.4821905072782229,\n",
       " 0.0,\n",
       " 0.48549267836390797,\n",
       " 0.5642277580477253,\n",
       " 0.0,\n",
       " 0.6792038922134029,\n",
       " 0.5223072889605395,\n",
       " 0.6024113460265765,\n",
       " 0.5424202560993177,\n",
       " 0.5709456970692327,\n",
       " 0.533735985126874,\n",
       " 0.40233798604952886,\n",
       " 0.5108914458667259,\n",
       " 0.6428478407429326,\n",
       " 0.9238487481558492,\n",
       " 0.629965452040583,\n",
       " 0.4641877387136098,\n",
       " 0.39364794685816185,\n",
       " 0.5545993015166203,\n",
       " 0.5057683545717043,\n",
       " 0.5246308888433007,\n",
       " 0.6222292953766146,\n",
       " 0.4983968065669408,\n",
       " 0.5582400991726444,\n",
       " 0.5998918658525131,\n",
       " 0.6433333558831856,\n",
       " 0.6399584146975642,\n",
       " 0.48175867857399945,\n",
       " 0.5005968522088858,\n",
       " 0.5299266101246505,\n",
       " 0.8349998728140104,\n",
       " 0.4785947037126745,\n",
       " 0.625251502136752,\n",
       " 0.5273963865247026,\n",
       " 0.3735238834837021,\n",
       " 0.5211186354136125,\n",
       " 0.4378060225405689,\n",
       " 0.4446294848456064,\n",
       " 0.3491899728502138,\n",
       " 0.6255186072088402,\n",
       " 0.4928160537290037,\n",
       " 0.574212363657299,\n",
       " 0.0,\n",
       " 0.5307975002346459,\n",
       " 0.54632603454468,\n",
       " 0.4348289842908944,\n",
       " 0.5591141060274346,\n",
       " 0.7729996551484652,\n",
       " 0.6370596861551125,\n",
       " 0.41180093551644564,\n",
       " 0.7239535198442213,\n",
       " 0.747548299679817,\n",
       " 0.6195392888029082,\n",
       " 0.0,\n",
       " 0.7213315638362884,\n",
       " 0.5977356307506484,\n",
       " 0.40112862942114497,\n",
       " 0.6092415607201952,\n",
       " 0.5922518898320124,\n",
       " 0.6218022396151498,\n",
       " 0.7404147253796696,\n",
       " 0.5567882088255847,\n",
       " 0.49253833014175835,\n",
       " 0.6230228070876835,\n",
       " 0.2905590541068382,\n",
       " 0.7410136279041746,\n",
       " 0.5667664655083444,\n",
       " 0.5846186944404079,\n",
       " 0.6033133475639803,\n",
       " 0.4590335247742111,\n",
       " 0.5953839474324609,\n",
       " 0.6088285393647792,\n",
       " 0.5212264205456026,\n",
       " 0.44416295615919266,\n",
       " 0.48430552719188885,\n",
       " 0.511571866041276,\n",
       " 0.5351275825818115,\n",
       " 0.5305317293002747,\n",
       " 0.3561798091275219,\n",
       " 0.5881599730631422,\n",
       " 0.47158502566817145,\n",
       " 0.3912701630355401,\n",
       " 0.4367591769028681,\n",
       " 0.6436183844279982,\n",
       " 0.5919267734027839,\n",
       " 0.4994587760451838,\n",
       " 0.54388367494869,\n",
       " 0.6710105692722735,\n",
       " 0.5090995883033852,\n",
       " 0.2974640091383552,\n",
       " 0.38775637120299394,\n",
       " 0.45929070392838717,\n",
       " 0.60184768724457,\n",
       " 0.5352349931345268,\n",
       " 0.5228866008324049,\n",
       " 0.0,\n",
       " 0.7044693536793123,\n",
       " 0.43300508175144237,\n",
       " 0.457164738027929,\n",
       " 0.0,\n",
       " 0.4410415480242152,\n",
       " 0.42278057769160815,\n",
       " 0.0,\n",
       " 0.697579259183151,\n",
       " 0.4244133920517722,\n",
       " 0.5978370623730221,\n",
       " 0.5336699789726745,\n",
       " 0.6840264887903271,\n",
       " 0.5101390622148553,\n",
       " 0.4680288077821174,\n",
       " 0.5322335271025974,\n",
       " 0.549847308134429,\n",
       " 0.5959371123494203,\n",
       " 0.0,\n",
       " 0.5986348831123197,\n",
       " 0.5490113845031357,\n",
       " 0.4536917430564832,\n",
       " 0.3837824870938847,\n",
       " 0.7516387967257361,\n",
       " 0.7181458534492475,\n",
       " 0.5169491417858358,\n",
       " 0.5234345613257396,\n",
       " 0.6607356684580086,\n",
       " 0.5194927363674613,\n",
       " 0.5736672817417291,\n",
       " 0.4199364618038402,\n",
       " 0.5061646596011443,\n",
       " 0.5475075554257116,\n",
       " 0.6470947999386241,\n",
       " 0.569353498549493,\n",
       " 0.778879582658538,\n",
       " 0.35195667162611816,\n",
       " 0.6770817449633482,\n",
       " 0.5161944089088064,\n",
       " 0.5842361491871347,\n",
       " 0.5335928826058448,\n",
       " 0.5515849184301069,\n",
       " 0.4279215536488308,\n",
       " 0.5736384596413634,\n",
       " 0.4329489630389775,\n",
       " 0.7450235574217692,\n",
       " 0.6592453624238793,\n",
       " 0.6232635382162758,\n",
       " 0.5083580989143273,\n",
       " 0.4486334403194673,\n",
       " 0.4364946717601532,\n",
       " 0.3844012586023628,\n",
       " 0.3876396294165777,\n",
       " 0.6737096595919339,\n",
       " 0.5440513967331697,\n",
       " 0.5880915560583333,\n",
       " 0.6260246783610415,\n",
       " 0.5808929175910957,\n",
       " 0.4318157551797888,\n",
       " 0.5326601471734469,\n",
       " 0.4101994273512499,\n",
       " 0.5632601382322169,\n",
       " 0.6659977797644127,\n",
       " 0.5741094533823629,\n",
       " 0.5519299319259553,\n",
       " 0.5704923525628016,\n",
       " 0.5920167793058912,\n",
       " 0.4972964512495008,\n",
       " 0.4347744979431318,\n",
       " 0.595388003119733,\n",
       " 0.4734789720367716,\n",
       " 0.5122293095180322,\n",
       " 0.4595833187116511,\n",
       " 0.507490040525131,\n",
       " 0.6101616891204872,\n",
       " 0.6106039863925307,\n",
       " 0.5889166112716692,\n",
       " 0.6763888581206515,\n",
       " 0.5837925415157327,\n",
       " 0.5836652095347319,\n",
       " 0.2927669587224807,\n",
       " 0.5905124643258277,\n",
       " 0.49047002592360495,\n",
       " 0.39552759443936825,\n",
       " 0.41076468707426583,\n",
       " 0.6547076847514024,\n",
       " 0.6582353947693852,\n",
       " 0.37859581878331644,\n",
       " 0.3617524773296879,\n",
       " 0.5346093978565022,\n",
       " 0.4543306609671317,\n",
       " 0.545486233087821,\n",
       " 0.5296218145714284,\n",
       " 0.6416717926409384,\n",
       " 0.6068188272972808,\n",
       " 0.6659226769872115,\n",
       " 0.6548426837107235,\n",
       " 0.499802404365432,\n",
       " 0.4736826452277368,\n",
       " 0.44153214391313567,\n",
       " 0.557861708429274,\n",
       " 0.48479283564809755,\n",
       " 0.45976550203696603,\n",
       " 0.48800840072839524,\n",
       " 0.4736826452277368,\n",
       " 0.46267915062371245,\n",
       " 0.4195889917628089,\n",
       " 0.6378829233694099,\n",
       " 0.4312989543824533,\n",
       " 0.4279215536488308,\n",
       " 0.3233597107871684,\n",
       " 0.5831136841008434,\n",
       " 0.5535868298783335,\n",
       " 0.4820891715874391,\n",
       " 0.3799338527896167,\n",
       " 0.5413019299215841,\n",
       " 0.6481349284868911,\n",
       " 0.5746052734845847,\n",
       " 0.9007118881495383,\n",
       " 0.5714316033375877,\n",
       " 0.7280708980251269,\n",
       " 0.5086557628484321,\n",
       " 0.890007180075658,\n",
       " 0.5624501164926194,\n",
       " 0.5024300436136425,\n",
       " 0.5592816159294463,\n",
       " 0.5402916860912712,\n",
       " 0.4450925189023143,\n",
       " 0.4511934234318705,\n",
       " 0.7662891206022864,\n",
       " 0.7505925472473846,\n",
       " 0.6640816027724676,\n",
       " 0.5407841580837054,\n",
       " 0.21024419147420684,\n",
       " 0.7203096592474972,\n",
       " 0.598151998133689,\n",
       " 0.8840167981534968,\n",
       " 0.5313142050336872,\n",
       " 0.48157998619156417,\n",
       " 0.4896902294898748,\n",
       " 0.0,\n",
       " 0.5896106884263314,\n",
       " 0.5319172806223986,\n",
       " 0.0,\n",
       " 0.5433307077717864,\n",
       " 0.7025279158589172,\n",
       " 0.761195146464256,\n",
       " 0.7647429888281944,\n",
       " 0.714371840817487,\n",
       " 0.5006532096881288,\n",
       " 0.0,\n",
       " 0.6397648545739697,\n",
       " 0.536991272445831,\n",
       " 0.417821725669439,\n",
       " 0.7543628595444671,\n",
       " 0.6542649351059568,\n",
       " 0.6244381217613559,\n",
       " 0.6529061404744543,\n",
       " 0.6197037044265735,\n",
       " 0.6048530270041322,\n",
       " 0.6200910879958986,\n",
       " 0.6976475439263937,\n",
       " 0.5516178599483575,\n",
       " 0.0,\n",
       " 0.5408331901776021,\n",
       " 0.6368962014899178,\n",
       " 0.6216645321041757,\n",
       " 0.5473313193618322,\n",
       " 0.607662499346026,\n",
       " 0.4972369472267434,\n",
       " 0.45844163747492467,\n",
       " 0.48454966162271107,\n",
       " 0.6790175737785307,\n",
       " 0.0,\n",
       " 0.739599055071094,\n",
       " 0.6704592046366638,\n",
       " 0.5287817503868503,\n",
       " 0.6626953002586712,\n",
       " 0.5100100798883821,\n",
       " 0.40270538539280265,\n",
       " 0.5732973400650778,\n",
       " 0.5632805942770347,\n",
       " 0.44458299010288455,\n",
       " 0.5805999682854188,\n",
       " 0.6455740103002731,\n",
       " 0.539164030701345,\n",
       " 0.7446806160230846,\n",
       " 0.6188189479482654,\n",
       " 0.5054216640525232,\n",
       " 0.5048979549820675,\n",
       " 0.40565589048497297,\n",
       " 0.46683971677310226,\n",
       " 0.3925784607076384,\n",
       " 0.5693102260538271,\n",
       " 0.5604630550195018,\n",
       " 0.4486334403194673,\n",
       " 0.576900749933587,\n",
       " 0.5856147241678444,\n",
       " 0.4920007136242653,\n",
       " 0.41359279307978625,\n",
       " 0.49849410669333377,\n",
       " 0.4318729116864356,\n",
       " 0.6090370210380665,\n",
       " 0.6847826508572433,\n",
       " 0.5059409733868523,\n",
       " 0.40565589048497297,\n",
       " 0.43164396787830167,\n",
       " 0.3599501701411792,\n",
       " 0.5232915166983468,\n",
       " 0.6477293229358573,\n",
       " 0.6139878434145833,\n",
       " 0.3523715373278828,\n",
       " 0.24057192809530722,\n",
       " 0.6855447687120559,\n",
       " 0.47379872931356015,\n",
       " 0.49408192917866606,\n",
       " 0.33284125675167864,\n",
       " 0.0,\n",
       " 0.6684263296310952,\n",
       " 0.47934139666214465,\n",
       " 0.5071866959590104,\n",
       " 0.393541827076675,\n",
       " 0.42244877465375186,\n",
       " 0.5883829423592708,\n",
       " 0.5013626589694352,\n",
       " 0.49091124662295715,\n",
       " 0.5833555821948836,\n",
       " 0.3422889751028782,\n",
       " 0.5162673811254432,\n",
       " 0.6676676878624138,\n",
       " 0.36031486003399193,\n",
       " 0.4336745622037536,\n",
       " 0.2938420250813162,\n",
       " 0.41076468707426583,\n",
       " 0.3474237167606433,\n",
       " 0.27746396885524605,\n",
       " 0.42271435941260893,\n",
       " 0.33396519239721983,\n",
       " 0.0,\n",
       " 0.5154155605809472,\n",
       " 0.37188729611581867,\n",
       " 0.7080152798039404,\n",
       " 0.47559887026150743,\n",
       " 0.48083413650700396,\n",
       " 0.5740808367633922,\n",
       " 0.6489569978471355,\n",
       " 0.5205904745613334,\n",
       " 0.45844163747492467,\n",
       " 0.6957552643502382,\n",
       " 0.531905956127078,\n",
       " 0.3635037207276158,\n",
       " 0.48876406377794834,\n",
       " 0.6123606851725694,\n",
       " 0.5281110159583237,\n",
       " 0.5392344863472923,\n",
       " 0.4788356972246823,\n",
       " 0.43934378237623145,\n",
       " 0.549728580343027,\n",
       " 0.5122293095180322,\n",
       " 0.6247188512533821,\n",
       " 0.5063704232004067,\n",
       " 0.6830953294795274,\n",
       " 0.7015702449828779,\n",
       " 0.632223194418973,\n",
       " 0.5762961729324667,\n",
       " 0.3319846875139363,\n",
       " 0.5916128721821677,\n",
       " 0.5407939676143988,\n",
       " 0.5156222497167889,\n",
       " 0.48980223306741827,\n",
       " 0.6833675553510865,\n",
       " 0.30135914046621365,\n",
       " 0.17513439590616536,\n",
       " 0.605372802118338,\n",
       " 0.3783247009910913,\n",
       " 0.3233597107871684,\n",
       " 0.5615592781785914,\n",
       " 0.5244650458281389,\n",
       " 0.6186020343242197,\n",
       " 0.5711381167033145,\n",
       " 0.3791345128900259,\n",
       " 0.621685737853109,\n",
       " 0.7359753141480404,\n",
       " 0.5415155867981978,\n",
       " 0.0,\n",
       " 0.6465175743979963,\n",
       " 0.5118388170158173,\n",
       " 0.4678375293699716,\n",
       " 0.45663119180212675,\n",
       " 0.7637011572506499,\n",
       " 0.7138465536400845,\n",
       " 0.5051603641388395,\n",
       " 0.3523715373278828,\n",
       " 0.5881234934176253,\n",
       " 0.5038726938646851,\n",
       " 0.6682618722531358,\n",
       " 0.41710042509858547,\n",
       " 0.5760415858731109,\n",
       " 0.5218159025446261,\n",
       " 0.6114229037480282,\n",
       " 0.4475790052421943,\n",
       " 0.5263520897232964,\n",
       " 0.7113650592980009,\n",
       " 0.3892542857795573,\n",
       " 0.5261410964699385,\n",
       " 0.4761874532203714,\n",
       " 0.3563738901583364,\n",
       " 0.8164551962624276,\n",
       " 0.6457107169345637,\n",
       " 0.4343917428035961,\n",
       " 0.7532501935672908,\n",
       " 0.6777487629859485,\n",
       " 0.5495670841503562,\n",
       " 0.5081754850221822,\n",
       " 0.4258154363464307,\n",
       " 0.5662942738856284,\n",
       " 0.4995735311973263,\n",
       " 0.5464783208151789,\n",
       " 0.3678375265623114,\n",
       " 0.4641877387136098,\n",
       " 0.378052388843504,\n",
       " 0.6161585574430539,\n",
       " 0.6093581373837604,\n",
       " 0.5314170297164507,\n",
       " 0.47400135337826776,\n",
       " 0.49757338182643135,\n",
       " 0.4355867929109187,\n",
       " 0.37483141241592766,\n",
       " 0.42055702788332416,\n",
       " 0.3325570504692608,\n",
       " 0.5685198327827027,\n",
       " 0.4610259689626938,\n",
       " 0.5590411338107978,\n",
       " 0.46312871447824583,\n",
       " 0.4006571186869724,\n",
       " 0.5954366506229267,\n",
       " 0.4423090874645511,\n",
       " 0.3432548503404556,\n",
       " 0.5122604426123869,\n",
       " 0.4821905072782229,\n",
       " 0.6440610116521936,\n",
       " 0.3523715373278828,\n",
       " 0.4710114047542515,\n",
       " 0.3230282635719703,\n",
       " 0.436811942384121,\n",
       " 0.5237971882217487,\n",
       " 0.6367364384965576,\n",
       " 0.5302182722564525,\n",
       " 0.5068139084624608,\n",
       " 0.441727297216615,\n",
       " 0.5222411132155035,\n",
       " 0.5855006133608199,\n",
       " 0.30720780410470894,\n",
       " 0.5625674710287425,\n",
       " 0.4321010106815487,\n",
       " 0.0,\n",
       " 0.3604964013026102,\n",
       " 0.0,\n",
       " 0.46236598765849557,\n",
       " 0.5056991718476004,\n",
       " 0.45106791119070966,\n",
       " 0.0,\n",
       " 0.3946992435842872,\n",
       " 0.5867443940606498,\n",
       " 0.44402230818603533,\n",
       " 0.5268457218736867,\n",
       " 0.4455978820536676,\n",
       " 0.4159285462571034,\n",
       " 0.6325219462631936,\n",
       " 0.5348798277109298,\n",
       " 0.471162874463794,\n",
       " 0.5594051364969164,\n",
       " 0.4401485424168854,\n",
       " 0.4102804958973409,\n",
       " 0.5530823475491885,\n",
       " 0.5552237771651345,\n",
       " 0.4719445890137438,\n",
       " 0.3513291052456703,\n",
       " 0.7111129634190377,\n",
       " 0.5752770875904336,\n",
       " 0.0,\n",
       " 0.38252604632740334,\n",
       " 0.579691789034097,\n",
       " 0.5021374288303785,\n",
       " 0.4938308194594736,\n",
       " 0.6382103899057063,\n",
       " 0.4602727772199573,\n",
       " 0.6286985054944896,\n",
       " 0.4519824703518318,\n",
       " 0.5620341340510733,\n",
       " 0.4394449522147468,\n",
       " 0.6907150160178557,\n",
       " 0.5202084796994854,\n",
       " 0.4581061932885808,\n",
       " 0.0,\n",
       " 0.5031014566518061,\n",
       " 0.39438572765845364,\n",
       " 0.7308006551639619,\n",
       " 0.5576977632257876,\n",
       " 0.7655564450861878,\n",
       " 0.7401906782907842,\n",
       " 0.0,\n",
       " 0.6921005069481738,\n",
       " 0.5389824894327266,\n",
       " 0.7007193831717228,\n",
       " 0.6363216217600716,\n",
       " 0.6961490308683775,\n",
       " 0.6346274960308441,\n",
       " 0.0,\n",
       " 0.48098919032793713,\n",
       " 0.6590603227870047,\n",
       " 0.5253906045617026,\n",
       " 0.5383836717304541,\n",
       " 0.6085866026272289,\n",
       " 0.48279505740285333,\n",
       " 0.5884555748212185,\n",
       " 0.44472236954758293,\n",
       " 0.5854148918035064,\n",
       " 0.5939644593806196,\n",
       " 0.7946119967136249,\n",
       " 0.68776721770212,\n",
       " 0.6346532862088975,\n",
       " 0.5623394612694527,\n",
       " 0.40388457279805806,\n",
       " 0.38315745894233794,\n",
       " 0.5170212289748038,\n",
       " 0.4850592267882446,\n",
       " 0.5091323378445668,\n",
       " 0.39236232497158563,\n",
       " 0.0,\n",
       " 0.5270788233493111,\n",
       " 0.5527194655352778,\n",
       " 0.5493539443021479,\n",
       " 0.7164723679180414,\n",
       " 0.46109722517523494,\n",
       " 0.5569016928991966,\n",
       " 0.873038882554039,\n",
       " 0.5868049990542256,\n",
       " 0.48471998383349607,\n",
       " 0.5435374777176236,\n",
       " 0.5220421598492464,\n",
       " 0.8142437966052231,\n",
       " 0.6783313275757178,\n",
       " 0.6424551490073054,\n",
       " 0.5344356997933946,\n",
       " 0.48903657600858436,\n",
       " 0.4766875303907078,\n",
       " 0.0,\n",
       " 0.7458959561500011,\n",
       " 0.0,\n",
       " 0.6733660142909779,\n",
       " 0.5828410334591733,\n",
       " 0.0,\n",
       " 0.5552470727212928,\n",
       " 0.0,\n",
       " 0.489936370026096,\n",
       " 0.6671338390204461,\n",
       " 0.5025029813891071,\n",
       " 0.4887185280428988,\n",
       " 0.7782739490787453,\n",
       " 0.34787003463846294,\n",
       " 0.5907577265451265,\n",
       " 0.5801345713925369,\n",
       " 0.5005216288865094,\n",
       " 0.5938231969346707,\n",
       " 0.5430854482258856,\n",
       " 0.4764379975029336,\n",
       " 0.3213438335311857,\n",
       " 0.31890575242769653,\n",
       " 0.6297936797912007,\n",
       " 0.580455553266795,\n",
       " 0.6420485090080135,\n",
       " 0.49412368180270155,\n",
       " 0.6220583000876729,\n",
       " 0.5825571934970124,\n",
       " 0.5454680469277797,\n",
       " 0.568010393529507,\n",
       " 0.5169347142545452,\n",
       " 0.5012325812714303,\n",
       " 0.3569525036579339,\n",
       " 0.5363305943905805,\n",
       " 0.6117879065311077,\n",
       " 0.4455063046183486,\n",
       " 0.7450876701582072,\n",
       " 0.6597685616534987,\n",
       " 0.43547910166846393,\n",
       " 0.6654205171331099,\n",
       " 0.6098639866496944,\n",
       " 0.5519053518643148,\n",
       " 0.43118352062802195,\n",
       " 0.5447821745439407,\n",
       " 0.4485026153869644,\n",
       " 0.4750898712118909,\n",
       " 0.6509399908521294,\n",
       " 0.557824486318476,\n",
       " 0.5388206706033151,\n",
       " 0.6102446619618502,\n",
       " 0.5457674365199505,\n",
       " 0.47635459552374076,\n",
       " 0.37188729611581867,\n",
       " 0.5463350029534784,\n",
       " 0.7121465270522227,\n",
       " 0.4921517181672499,\n",
       " 0.5807902863911384,\n",
       " 0.5219357889775956,\n",
       " 0.5828807665523199,\n",
       " 0.5697845655511602,\n",
       " 0.5197414326429353,\n",
       " 0.47827191051162116,\n",
       " 0.43112572281405476,\n",
       " 0.41289780410019084,\n",
       " 0.6078484285023125,\n",
       " 0.523564315272489,\n",
       " 0.45156843827794435,\n",
       " 0.4616290445514051,\n",
       " 0.7803014414186752,\n",
       " 0.5769608836843279,\n",
       " 0.5805432740443264,\n",
       " 0.581975497643306,\n",
       " 0.6818001610847808,\n",
       " 0.6676979108177864,\n",
       " 0.6477397563933256,\n",
       " 0.5038191977492438,\n",
       " 0.0,\n",
       " 0.6516589469851684,\n",
       " 0.0,\n",
       " 0.48170767629591266,\n",
       " 0.5942666719036913,\n",
       " 0.5571807327226098,\n",
       " 0.6038249502937667,\n",
       " 0.4814520317730044,\n",
       " 0.5493710225610036,\n",
       " 0.4374938261318292,\n",
       " 0.7467660244639595,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.7624805523177759,\n",
       " 0.6319252374136288,\n",
       " 0.37020619786814296,\n",
       " 0.5542215653492502,\n",
       " 0.0,\n",
       " 0.6566346298465148,\n",
       " 0.43929313529635233,\n",
       " 0.6780263522630867,\n",
       " 0.6960774786407913,\n",
       " 0.545830757415623,\n",
       " 0.74953263948508,\n",
       " 0.5045463342527654,\n",
       " 0.6079577325896613,\n",
       " 0.6313703152344196,\n",
       " 0.48398670408957584,\n",
       " 0.6627498804198564,\n",
       " 0.6367094247353763,\n",
       " 0.5544028519851851,\n",
       " 0.6637483326946337,\n",
       " 0.5953514923513009,\n",
       " 0.0,\n",
       " 0.5386787307991938,\n",
       " 0.6509614505841742,\n",
       " 0.42171247755327185,\n",
       " 0.7468983167190688,\n",
       " 0.530206632155197,\n",
       " 0.5744632031909861,\n",
       " 0.4442097677915202,\n",
       " 0.6955157076789938,\n",
       " 0.635906108177879,\n",
       " 0.6120857945993566,\n",
       " 0.36863721116314896,\n",
       " 0.6459528080830401,\n",
       " 0.4670664413148,\n",
       " 0.5920767105159517,\n",
       " 0.6311137325896298,\n",
       " 0.612443837837049,\n",
       " 0.5175377898432564,\n",
       " 0.7415084458022037,\n",
       " 0.6008125361593687,\n",
       " 0.6552505402335734,\n",
       " 0.6015070082910047,\n",
       " 0.6691081283298822,\n",
       " 0.5298329868465723,\n",
       " 0.3922539730463245,\n",
       " 0.49895902714909335,\n",
       " 0.0,\n",
       " 0.7717927199510637,\n",
       " 0.33053015552186044,\n",
       " 0.0,\n",
       " 0.6978140593768692,\n",
       " 0.6270051074394172,\n",
       " 0.4810665716326817,\n",
       " 0.5618809030243541,\n",
       " 0.4662856129425754,\n",
       " 0.6554626717560039,\n",
       " 0.5151042252524091,\n",
       " 0.7819462828161668,\n",
       " 0.6294463591243541,\n",
       " 0.6087534441793495,\n",
       " 0.6720635764227322,\n",
       " 0.4290081691636795,\n",
       " 0.5075068494327899,\n",
       " 0.44158099002988865,\n",
       " 0.7939838730876422,\n",
       " 0.5803728786644391,\n",
       " 0.7185647049630733,\n",
       " 0.41473402101781,\n",
       " 0.4055685197883999,\n",
       " 0.6828209137004475,\n",
       " 0.5689753875475824,\n",
       " 0.8134522069300844,\n",
       " 0.6384997976839273,\n",
       " 0.5381384153034436,\n",
       " 0.6590125273090759,\n",
       " 0.5709758021482549,\n",
       " 0.6828199279370614,\n",
       " 0.648508703290238,\n",
       " 0.6386490308262758,\n",
       " 0.3089014650508395,\n",
       " 0.5956712420146737,\n",
       " 0.5266117380514955,\n",
       " 0.39892926452473054,\n",
       " 0.6461315255590234,\n",
       " 0.4481962761385927,\n",
       " 0.2589824920313661,\n",
       " 0.6047276279263455,\n",
       " 0.4312989543824533,\n",
       " 0.6290076802312669,\n",
       " 0.5034434229899917,\n",
       " 0.4878700094223794,\n",
       " 0.47715613949248403,\n",
       " 0.4523527428851216,\n",
       " 0.5767090251564048,\n",
       " 0.5260415557760704,\n",
       " 0.4898246094504805,\n",
       " 0.47721103714932,\n",
       " 0.528411350615478,\n",
       " 0.6375455113139942,\n",
       " 0.36333081220219066,\n",
       " 0.41060371105221244,\n",
       " 0.537706861054534,\n",
       " 0.3800660769203201,\n",
       " 0.5335487843285799,\n",
       " 0.44605374516345603,\n",
       " 0.4653952276078148,\n",
       " 0.5666955437448536,\n",
       " 0.6478873748317336,\n",
       " 0.4747767402945983,\n",
       " 0.45393211182676674,\n",
       " 0.4405470273264847,\n",
       " 0.5216824221266737,\n",
       " 0.615385735320761,\n",
       " 0.4777301475561879,\n",
       " 0.453005578075523,\n",
       " 0.44880744397422206,\n",
       " 0.4121949123602028,\n",
       " 0.3342430119998574,\n",
       " 0.5160628414433144,\n",
       " 0.5042990042270381,\n",
       " 0.6359839671532547,\n",
       " 0.3446759418353489,\n",
       " 0.507203587544724,\n",
       " 0.5238100999204974,\n",
       " 0.5313142050336872,\n",
       " 0.5253276503271804,\n",
       " 0.5805690506001122,\n",
       " 0.673125473965207,\n",
       " 0.5144317505701628,\n",
       " 0.6798081651546916,\n",
       " 0.5887542848765963,\n",
       " 0.6239980304745285,\n",
       " 0.9529684739032808,\n",
       " 0.6941866209355074,\n",
       " 0.6009312965369831,\n",
       " 0.5766376613681892,\n",
       " 0.7220406873276841,\n",
       " 0.5587116885957466,\n",
       " 0.6192201731910243,\n",
       " 0.5574809871443245,\n",
       " 0.5138126162727649,\n",
       " 0.7545098574526242,\n",
       " 0.7017686290713212,\n",
       " 0.5972259609336765,\n",
       " 0.37308182135511203,\n",
       " 0.5899570402474487,\n",
       " 0.6401592335915334,\n",
       " 0.0,\n",
       " 0.5160628414433144,\n",
       " 0.0,\n",
       " 0.7668245633135427,\n",
       " 0.5343813189000003,\n",
       " 0.48754588635561497,\n",
       " 0.0,\n",
       " 0.5968151776466497,\n",
       " 0.46735672769111797,\n",
       " 0.45605437805963583,\n",
       " 0.4982798438877033,\n",
       " 0.6986842227788379,\n",
       " 0.6957832455007824,\n",
       " 0.3957329592712312,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.2264621748679656,\n",
       " 0.49747462040977736,\n",
       " 0.4538120442768105,\n",
       " 0.8935137661276769,\n",
       " 0.696222881262238,\n",
       " 0.5382815997826668,\n",
       " 0.41359279307978625,\n",
       " 0.44250180705178266,\n",
       " 0.5736845685535502,\n",
       " 0.5182905118724657,\n",
       " 0.5135083343020392,\n",
       " 0.4485898627656467,\n",
       " 0.600488498186267,\n",
       " 0.7390005229753225,\n",
       " 0.3342430119998574,\n",
       " 0.694457510795877,\n",
       " 0.5219091676345768,\n",
       " 0.46944483513241414,\n",
       " 0.4072928926209641,\n",
       " 0.49831885604269627,\n",
       " 0.623209261462084,\n",
       " 0.4995544201266734,\n",
       " 0.7536664646026712,\n",
       " 0.7260484434973449,\n",
       " 0.5541504821218669,\n",
       " 0.5447821745439407,\n",
       " 0.6278600034409005,\n",
       " 0.3439694797740768,\n",
       " 0.4875922938334212,\n",
       " 0.46847283565621234,\n",
       " 0.600808821218562,\n",
       " 0.6715393150520679,\n",
       " 0.637891080233808,\n",
       " 0.48336892203833126,\n",
       " 0.41084501815449176,\n",
       " 0.595731767582551,\n",
       " 0.0,\n",
       " 0.3995105988174728,\n",
       " 0.4830701791731685,\n",
       " 0.49936298374717525,\n",
       " 0.0,\n",
       " 0.39999090238027435,\n",
       " 0.37468748974896865,\n",
       " 0.8526841238728123,\n",
       " 0.7030090669823754,\n",
       " 0.5743778040189397,\n",
       " 0.5791615024753416,\n",
       " 0.46155839910653357,\n",
       " 0.6555531439871433,\n",
       " 0.629768105279021,\n",
       " 0.763390561212607,\n",
       " 0.6742000432734099,\n",
       " 0.6274098793881466,\n",
       " 0.5018802496762025,\n",
       " 0.4704630198685728,\n",
       " 0.6145139041944068,\n",
       " 0.38476957006459006,\n",
       " 0.38098381021980515,\n",
       " 0.4490675345018394,\n",
       " 0.0,\n",
       " 0.5318493025097547,\n",
       " 0.49782942547686587,\n",
       " 0.47997431779770755,\n",
       " 0.33451958855761055,\n",
       " 0.5122293095180322,\n",
       " 0.4857082696763467,\n",
       " 0.529903217595503,\n",
       " 0.39300847814639117,\n",
       " 0.798839042196018,\n",
       " 0.6284733235070344,\n",
       " 0.48517993519905717,\n",
       " 0.0,\n",
       " 0.7278605215917173,\n",
       " 0.7048956926808757,\n",
       " 0.6725031083205807,\n",
       " 0.5720499344319511,\n",
       " 0.671009375971741,\n",
       " 0.44664138530960157,\n",
       " 0.8253256419521364,\n",
       " 0.581920051614157,\n",
       " 0.4064367188571976,\n",
       " 0.4630252587929514,\n",
       " 0.5287102331121534,\n",
       " 0.4978687228447084,\n",
       " 0.7797893727722947,\n",
       " 0.3230282635719703,\n",
       " 0.44331422761962386,\n",
       " 0.5707466349797967,\n",
       " 0.5459211034672722,\n",
       " 0.5796708738731684,\n",
       " 0.5937149544705969,\n",
       " 0.5434435771108772,\n",
       " 0.0,\n",
       " 0.6184849186352509,\n",
       " 0.4575053160659823,\n",
       " 0.44235732371946423,\n",
       " 0.5206040736720547,\n",
       " 0.5352993499935774,\n",
       " 0.5503451704522947,\n",
       " 0.4079693946675227,\n",
       " 0.5517823047315187,\n",
       " 0.5839440184093325,\n",
       " 0.3446759418353489,\n",
       " 0.49420710254077976,\n",
       " 0.4086385731380875,\n",
       " 0.7707970883589157,\n",
       " 0.51728004899368,\n",
       " 0.7056851954270735,\n",
       " 0.47191470515648576,\n",
       " 0.4219812377920173,\n",
       " 0.5733610815596294,\n",
       " 0.5840415488138101,\n",
       " 0.5111930205804741,\n",
       " 0.45189988549314253,\n",
       " 0.3655417544726919,\n",
       " 0.3542047287326394,\n",
       " 0.561461059271821,\n",
       " 0.4961559621318084,\n",
       " 0.5547716621689471,\n",
       " 0.33588420815923853,\n",
       " 0.4279215536488308,\n",
       " 0.4319870663957655,\n",
       " 0.28084266537349134,\n",
       " 0.45954681752515536,\n",
       " 0.41366953394319245,\n",
       " 0.4769636128395229,\n",
       " 0.5579286517987685,\n",
       " 0.33929735466399746,\n",
       " 0.6672797767631982,\n",
       " 0.7762442761802396,\n",
       " 0.5111455012257692,\n",
       " 0.6254887088974789,\n",
       " 0.3971517209032241,\n",
       " 0.573262544253557,\n",
       " 0.5335156899630882,\n",
       " 0.6495288815546796,\n",
       " 0.32269502957070045,\n",
       " 0.32964071920503224,\n",
       " 0.0,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'word': words,\n",
    "              'frequency': frequencies_,\n",
    "              'probability': weights.tolist()}).to_csv('word_frequencies_and_probabilities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2869"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 100\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-25 07:25:13.806606: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 938us/step\n",
      "90/90 [==============================] - 0s 915us/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 948us/step\n",
      "90/90 [==============================] - 0s 944us/step\n",
      "90/90 [==============================] - 0s 894us/step\n",
      "90/90 [==============================] - 0s 942us/step\n",
      "90/90 [==============================] - 0s 929us/step\n",
      "90/90 [==============================] - 0s 960us/step\n",
      "90/90 [==============================] - 0s 923us/step\n",
      "90/90 [==============================] - 0s 919us/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 970us/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 955us/step\n",
      "90/90 [==============================] - 0s 937us/step\n",
      "90/90 [==============================] - 0s 903us/step\n",
      "90/90 [==============================] - 0s 948us/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 975us/step\n",
      "90/90 [==============================] - 0s 901us/step\n",
      "90/90 [==============================] - 0s 913us/step\n",
      "90/90 [==============================] - 0s 1ms/step\n",
      "90/90 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "hidden = 90\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 80\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "    pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "    afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "    efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "    \n",
    "    with open(PATH + sfn, 'w') as f:\n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                \"hidden_units\",\n",
    "                                                \"learning_rate\",\n",
    "                                                \"batch_size\",\n",
    "                                                \"epochs\",\n",
    "                                                \"loss_train\",\n",
    "                                                \"accuracy_train\",\n",
    "                                                \"mse_train\",\n",
    "                                                \"loss_test\",\n",
    "                                                \"accuracy_test\",\n",
    "                                                \"mse_test\",\n",
    "                                                \"loss_holdout\",\n",
    "                                                \"accuracy_holdout\",\n",
    "                                                \"mse_holdout\"))\n",
    "\n",
    "        model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "        model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "        loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "        loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "        loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "        preds = (model.predict(X) > .5).astype(int)\n",
    "        np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "        accuracies = (preds == Y).astype(int)\n",
    "        np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "        mse = K.eval(K.mean(K.square(model.predict(X) - Y), axis = 1))\n",
    "        np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "        \n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    cfg['learning_rate'],\n",
    "                                                    cfg['batch_size'],\n",
    "                                                    cfg['epochs'],\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    loss_holdout,\n",
    "                                                    accuracy_holdout,\n",
    "                                                    mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8594.5916 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "hidden = 70\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample > 8029:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "        \n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(model.predict(X) - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "            \n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 60 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 60\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample == 9664:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "        \n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(model.predict(X) - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "                    \n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "    end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 50\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 40 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 40\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "\n",
    "    sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "    pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "    afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "    efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "\n",
    "    with open(PATH + sfn, 'w') as f:\n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                \"hidden_units\",\n",
    "                                                \"learning_rate\",\n",
    "                                                \"batch_size\",\n",
    "                                                \"epochs\",\n",
    "                                                \"loss_train\",\n",
    "                                                \"accuracy_train\",\n",
    "                                                \"mse_train\",\n",
    "                                                \"loss_test\",\n",
    "                                                \"accuracy_test\",\n",
    "                                                \"mse_test\",\n",
    "                                                \"loss_holdout\",\n",
    "                                                \"accuracy_holdout\",\n",
    "                                                \"mse_holdout\"))\n",
    "\n",
    "        model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "        model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "        loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "        loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "        loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "        preds = (model.predict(X) > .5).astype(int)\n",
    "        np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "        accuracies = (preds == Y).astype(int)\n",
    "        np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "        mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "        np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "        \n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    cfg['learning_rate'],\n",
    "                                                    cfg['batch_size'],\n",
    "                                                    cfg['epochs'],\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    loss_holdout,\n",
    "                                                    accuracy_holdout,\n",
    "                                                    mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 30 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6768.444 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "hidden = 30\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample > 8256:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "            \n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Hidden Units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 20\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample > 5810:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 15\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample > 7709:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 10\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    if sample > 5136:\n",
    "        sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "        pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "        afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "        efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "        with open(PATH + sfn, 'w') as f:\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    \"hidden_units\",\n",
    "                                                    \"learning_rate\",\n",
    "                                                    \"batch_size\",\n",
    "                                                    \"epochs\",\n",
    "                                                    \"loss_train\",\n",
    "                                                    \"accuracy_train\",\n",
    "                                                    \"mse_train\",\n",
    "                                                    \"loss_test\",\n",
    "                                                    \"accuracy_test\",\n",
    "                                                    \"mse_test\",\n",
    "                                                    \"loss_holdout\",\n",
    "                                                    \"accuracy_holdout\",\n",
    "                                                    \"mse_holdout\"))\n",
    "\n",
    "            model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "            model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "            loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "            loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "            loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "            preds = (model.predict(X) > .5).astype(int)\n",
    "            np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "            accuracies = (preds == Y).astype(int)\n",
    "            np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "            np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "\n",
    "            f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                        hidden,\n",
    "                                                        cfg['learning_rate'],\n",
    "                                                        cfg['batch_size'],\n",
    "                                                        cfg['epochs'],\n",
    "                                                        loss_train,\n",
    "                                                        accuracy_train,\n",
    "                                                        mse_train,\n",
    "                                                        loss_test,\n",
    "                                                        accuracy_test,\n",
    "                                                        mse_test,\n",
    "                                                        loss_holdout,\n",
    "                                                        accuracy_holdout,\n",
    "                                                        mse_holdout))\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 5\n",
    "PATH = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for sample in range(samples.shape[1]):\n",
    "    sfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_summary.csv'\n",
    "    pfn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_preds.csv'\n",
    "    afn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_accuracies.csv'\n",
    "    efn = 'sample_' + str(sample) + '_hidden_' + str(hidden) + '_error.csv'\n",
    "\n",
    "    with open(PATH + sfn, 'w') as f:\n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                \"hidden_units\",\n",
    "                                                \"learning_rate\",\n",
    "                                                \"batch_size\",\n",
    "                                                \"epochs\",\n",
    "                                                \"loss_train\",\n",
    "                                                \"accuracy_train\",\n",
    "                                                \"mse_train\",\n",
    "                                                \"loss_test\",\n",
    "                                                \"accuracy_test\",\n",
    "                                                \"mse_test\",\n",
    "                                                \"loss_holdout\",\n",
    "                                                \"accuracy_holdout\",\n",
    "                                                \"mse_holdout\"))\n",
    "\n",
    "        model = learner(X, Y, cfg['seed'], hidden, optimizer=Adam(learning_rate=cfg['learning_rate']))\n",
    "        model.fit(X[samples[:, sample]], Y[samples[:, sample]], epochs=cfg['epochs'], batch_size=cfg['batch_size'], verbose=False, sample_weight = weights[samples[:, sample]])\n",
    "\n",
    "        loss_train, accuracy_train, mse_train = model.evaluate(X[samples[:, sample]], Y[samples[:, sample]], verbose=0) \n",
    "        loss_test, accuracy_test, mse_test = model.evaluate(X[tests[:, sample]], Y[tests[:, sample]], verbose=0) \n",
    "        loss_holdout, accuracy_holdout, mse_holdout = model.evaluate(X[holdouts[:, sample]], Y[holdouts[:, sample]], verbose=0) \n",
    "\n",
    "        preds = (model.predict(X) > .5).astype(int)\n",
    "        np.savetxt(PATH + pfn, preds, fmt='%d', delimiter=',')\n",
    "\n",
    "        accuracies = (preds == Y).astype(int)\n",
    "        np.savetxt(PATH + afn, np.mean(accuracies, axis = 1), delimiter=',', fmt='%0.5f')\n",
    "\n",
    "        mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "        np.savetxt(PATH + efn, mse, delimiter=',', fmt='%0.5f')\n",
    "\n",
    "        f.write(\"{},{},{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    cfg['learning_rate'],\n",
    "                                                    cfg['batch_size'],\n",
    "                                                    cfg['epochs'],\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    loss_holdout,\n",
    "                                                    accuracy_holdout,\n",
    "                                                    mse_holdout))\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(round(end-start, 4), \"seconds elapsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate MSE data for every word for every model after the fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = 'outputs/brute_force_2_with_frequency/'\n",
    "pattern = 'preds'\n",
    "filenames = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if pattern in filename:\n",
    "        filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "for file in filenames:\n",
    "\n",
    "    outfile = file.replace('preds', 'error')\n",
    "    preds = np.genfromtxt(directory + file, delimiter=',')\n",
    "    mse = K.eval(K.mean(K.square(preds - Y), axis = 1))\n",
    "    np.savetxt(directory + outfile, mse, delimiter=',', fmt='%0.5f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
