{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Set the print options\n",
    "np.set_printoptions(suppress=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.genfromtxt('data/samples.csv', delimiter=\",\").astype(bool)\n",
    "holdouts = np.genfromtxt('data/holdouts.csv', delimiter=\",\").astype(bool)\n",
    "tests = np.genfromtxt('data/tests.csv', delimiter=\",\").astype(bool)\n",
    "words = pd.read_csv('data/kidwords/kidwords.csv', names = [\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulate dichotomous accuracy per model and place in a file\n",
    "Extract the accuracy data for each sample and write to file. This takes a minute because it requires iterating through all the written accuracy files. Note that the accuracy summary datafile writes the accuracies out in a random order over rows, so you might need to arrange the rows if you want them to be in an orderly fashion.\n",
    "\n",
    "This routine was copied from elsewhere and needs to be inspected. The code doesn't look right to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory\n",
    "directory = 'outputs/brute_force_2_with_frequency/'\n",
    "\n",
    "# Prepare an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Specify the indices\n",
    "\n",
    "# Iterate over every file in the specified directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\"accuracies.csv\"):\n",
    "        # Extract the ID and level from the filename\n",
    "        id_start = filename.find(\"sample_\") + len(\"sample_\")\n",
    "        id_end = filename.find(\"hidden\")\n",
    "        ID = int(filename[id_start:id_end].replace(\"_\", \"\"))\n",
    "\n",
    "        level_start = filename.find(\"hidden_\") + len(\"hidden_\")\n",
    "        level_end = filename.find(\"accuracies\")\n",
    "        level = int(filename[level_start:level_end].replace(\"_\", \"\"))\n",
    "\n",
    "        # Read the csv file\n",
    "        data = np.genfromtxt(os.path.join(directory, filename), delimiter=',')\n",
    "\n",
    "        # Tabulate the number of values equal to one and less than one\n",
    "        equal_to_one = np.sum(data == 1)\n",
    "        less_than_one = np.sum(data < 1)\n",
    "\n",
    "        # Tabulate the values for each set of indices\n",
    "        # if you want to determine\n",
    "        train = np.sum(data[samples[:,ID]] == 1)/sum(samples[:,ID])\n",
    "        test = np.sum(data[tests[:,ID]] == 1)/sum(tests[:,ID])\n",
    "        holdout = np.sum(data[holdouts[:,ID]] == 1)/sum(holdouts[:,ID])\n",
    "\n",
    "        # Save the results\n",
    "        results.append([ID, level, train, test, holdout])\n",
    "\n",
    "# Convert the results to a numpy array and write to a csv file\n",
    "results_array = np.array(results)\n",
    "\n",
    "\n",
    "pd.DataFrame(results_array).to_csv('outputs/brute_force_2_with_frequency/tabulated_dichotomous_accuracy.csv', index = False, header = [\"model_id\", \"hidden\", \"train\", \"test\", \"holdout\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMEM subset data\n",
    "Generate a random sample of 200 model IDs to make the subset data for the LMEM (which DK is running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m345\u001b[39m)\n\u001b[1;32m      4\u001b[0m IDs \u001b[38;5;241m=\u001b[39m [d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m results_array]\n\u001b[0;32m----> 6\u001b[0m subset_models \u001b[38;5;241m=\u001b[39m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIDs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m subset_IDs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(ID))\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ID \u001b[38;5;129;01min\u001b[39;00m subset_models]\n",
      "File \u001b[0;32m~/miniconda3/envs/p39/lib/python3.9/random.py:449\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    447\u001b[0m randbelow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_randbelow\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    451\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(345)\n",
    "\n",
    "IDs = [d[0] for d in results_array]\n",
    "\n",
    "subset_models = random.sample(IDs, 200)\n",
    "subset_IDs = [str(int(ID)).zfill(4) for ID in subset_models]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the MSE (files labeled with \"error\") data for the LMEM subset (the LMEMs can't be run on the entire dataset). This routine does look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [str(i) for i in range(10000)]\n",
    "subset = random.sample(numbers, 200)\n",
    "\n",
    "\n",
    "filenames = []\n",
    "for filename in os.listdir(directory):\n",
    "    for ID in subset:\n",
    "        if filename.startswith(\"sample_\"+ID+\"_\"):\n",
    "            if \"error\" in filename:\n",
    "              if \"hidden_20\" in filename or \"hidden_100\" in filename:\n",
    "                  filenames.append(filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for filename in filenames:\n",
    "\n",
    "    id_start = filename.find(\"sample_\") + len(\"sample_\")\n",
    "    id_end = filename.find(\"hidden\")\n",
    "    ID = int(filename[id_start:id_end].replace(\"_\", \"\"))\n",
    "\n",
    "    level_start = filename.find(\"hidden_\") + len(\"hidden_\")\n",
    "    level_end = filename.find(\"error\")\n",
    "    level = int(filename[level_start:level_end].replace(\"_\", \"\"))\n",
    "\n",
    "    one_file = pd.read_csv(f'outputs/brute_force_2_with_frequency/{filename}', names = [\"mse\"])\n",
    "    one_file['word'] = words['word']\n",
    "    one_file['model_id'] = ID\n",
    "    one_file['hidden'] = level\n",
    "    one_file['train'] = samples[:,ID]\n",
    "    one_file['test'] = tests[:,ID]\n",
    "    one_file['holdout'] = holdouts[:,ID]\n",
    "    dfs.append(one_file)\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# this function needs to change if you are calculating accuracy from featurewise accuracy rather than from mse:\n",
    "df['accuracy'] = df['mse'].apply(lambda x: 0 if x > 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save file for LMEMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('outputs/brute_force_2_with_frequency/subset_data_for_lmem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabulate summary data per model\n",
    "Using the \"...summary.csv\" files, tabulate the summary data for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "cols_in = ['hidden_units', 'accuracy_train', 'mse_train', 'accuracy_test', 'mse_test', 'accuracy_holdout', 'mse_holdout']\n",
    "cols_out = ['model_id', 'hidden_units', 'accuracy_train', 'mse_train', 'accuracy_test', 'mse_test', 'accuracy_holdout', 'mse_holdout']\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\"summary.csv\"):\n",
    "        # Extract the ID and level from the filename\n",
    "        id_start = filename.find(\"sample_\") + len(\"sample_\")\n",
    "        id_end = filename.find(\"hidden\")\n",
    "        ID = int(filename[id_start:id_end].replace(\"_\", \"\"))\n",
    "\n",
    "        # Read the csv file\n",
    "        summary = pd.read_csv(os.path.join(directory, filename))[cols_in]\n",
    "        summary['model_id'] = ID\n",
    "\n",
    "        # Save the contents\n",
    "        results.append(summary)\n",
    "        \n",
    "\n",
    "pd.concat(results).reindex(columns=cols_out).to_csv('outputs/brute_force_2_with_frequency/model_summaries.csv', index = False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
