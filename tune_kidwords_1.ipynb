{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 20:24:18.986531: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-21 20:24:19.086042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from src.learner import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tuning for the `kidwords` set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs and outputs\n",
    "X = np.genfromtxt('data/kidwords/orth-kid.csv', delimiter=\",\")\n",
    "Y = np.genfromtxt('data/kidwords/phon-kid.csv', delimiter=\",\")\n",
    "words = pd.read_csv('data/kidwords/kidwords.csv', header=None)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tuning we will use a random sample of the same size that our samples will eventually be. This involves allocating 600 words for test, and the rest for train - but not using our pre-allocated samples for the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(982)\n",
    "\n",
    "target_train_size = 300\n",
    "\n",
    "train_n = X.shape[0]\n",
    "test_n = train_n-target_train_size\n",
    "\n",
    "sample = np.full(train_n, True, dtype=bool)\n",
    "\n",
    "indices = np.random.choice(train_n, test_n, replace=False)\n",
    "\n",
    "# Set chosen indices to True because they select the test items not the train items\n",
    "sample[indices] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited search across HPs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputs/tune_kidwords_1.csv', 'w') as f:\n",
    "    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                            \"hidden_units\",\n",
    "                                            \"learning_rate\",\n",
    "                                             \"batch_size\",\n",
    "                                             \"epochs\",\n",
    "                                             \"loss_train\",\n",
    "                                             \"accuracy_train\",\n",
    "                                             \"mse_train\",\n",
    "                                             \"loss_test\",\n",
    "                                             \"accuracy_test\",\n",
    "                                             \"mse_test\",\n",
    "                                             \"time\"))\n",
    "    for learning_rate in [.01, .025, .05, .075, .1, .15, .2, .25, None]: \n",
    "        for batch_size in [16, 32, 64, 96, 128, 256]:\n",
    "            for epochs in [50, 100, 150, 200, 250, 300]:\n",
    "                for hidden in [80, 100, 120]:\n",
    "\n",
    "                    if learning_rate is not None:\n",
    "                        optimizer = Adam(learning_rate=learning_rate)\n",
    "                    if learning_rate is None:\n",
    "                        optimzer = None\n",
    "\n",
    "                    model = learner(X, Y, seed, hidden, optimizer=None)\n",
    "                    \n",
    "                    start_time = time.time()\n",
    "\n",
    "\n",
    "                    model.fit(X[sample], Y[sample], epochs=epochs, batch_size=batch_size, verbose=False)\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    runtime = end_time - start_time\n",
    "\n",
    "                    loss_train, accuracy_train, mse_train = model.evaluate(X[sample], Y[sample], verbose=0) \n",
    "                    loss_test, accuracy_test, mse_test = model.evaluate(X[~sample], Y[~sample], verbose=0) \n",
    "\n",
    "                    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    learning_rate,\n",
    "                                                    batch_size,\n",
    "                                                    epochs,\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    runtime))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following configuration is the peak performer for models with 100 hidden units. These models are trivially different in performance than those for the 120 hidden unit versions, and outperform the 120 hidden unit versions on the holdout set By a very small amount. See `tune_kidwords.Rmd` for a summary of performance.\n",
    "\n",
    "train_accuracy = 0.997  \n",
    "test_accuracy = 0.986  \n",
    "time = 4.74 seconds\n",
    "\n",
    "Instead of 300 epochs, for speed we will go with 50...the differences in end performance are trivial (~ .0005 on binary accuracy difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 16\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get train and test indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.where(sample)[0]\n",
    "test_indices = np.where(~sample)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = []\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    if i in train_indices:\n",
    "        split.append('train')\n",
    "    elif i in test_indices:\n",
    "        split.append('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.3159 - binary_accuracy: 0.8951 - mse: 0.0976\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 996us/step - loss: 0.1216 - binary_accuracy: 0.9536 - mse: 0.0346\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0888 - binary_accuracy: 0.9664 - mse: 0.0257\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0678 - binary_accuracy: 0.9739 - mse: 0.0195\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0520 - binary_accuracy: 0.9812 - mse: 0.0145\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0412 - binary_accuracy: 0.9862 - mse: 0.0111\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 914us/step - loss: 0.0336 - binary_accuracy: 0.9891 - mse: 0.0088\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0283 - binary_accuracy: 0.9911 - mse: 0.0073\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0247 - binary_accuracy: 0.9925 - mse: 0.0063\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 924us/step - loss: 0.0214 - binary_accuracy: 0.9933 - mse: 0.0054\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0193 - binary_accuracy: 0.9937 - mse: 0.0049\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 955us/step - loss: 0.0180 - binary_accuracy: 0.9944 - mse: 0.0045\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0168 - binary_accuracy: 0.9948 - mse: 0.0043\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0157 - binary_accuracy: 0.9945 - mse: 0.0040\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 911us/step - loss: 0.0145 - binary_accuracy: 0.9955 - mse: 0.0037\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 953us/step - loss: 0.0141 - binary_accuracy: 0.9952 - mse: 0.0036\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0138 - binary_accuracy: 0.9952 - mse: 0.0036\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0131 - binary_accuracy: 0.9954 - mse: 0.0034\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 875us/step - loss: 0.0126 - binary_accuracy: 0.9957 - mse: 0.0033\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 902us/step - loss: 0.0124 - binary_accuracy: 0.9955 - mse: 0.0033\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 957us/step - loss: 0.0121 - binary_accuracy: 0.9959 - mse: 0.0032\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 958us/step - loss: 0.0117 - binary_accuracy: 0.9958 - mse: 0.0032\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 994us/step - loss: 0.0112 - binary_accuracy: 0.9961 - mse: 0.0030\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 887us/step - loss: 0.0110 - binary_accuracy: 0.9960 - mse: 0.0029\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 845us/step - loss: 0.0107 - binary_accuracy: 0.9960 - mse: 0.0029\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0107 - binary_accuracy: 0.9965 - mse: 0.0028\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0106 - binary_accuracy: 0.9963 - mse: 0.0029\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0101 - binary_accuracy: 0.9962 - mse: 0.0027\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0107 - binary_accuracy: 0.9960 - mse: 0.0029\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 894us/step - loss: 0.0101 - binary_accuracy: 0.9961 - mse: 0.0028\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 922us/step - loss: 0.0105 - binary_accuracy: 0.9959 - mse: 0.0029\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0103 - binary_accuracy: 0.9963 - mse: 0.0028\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0095 - binary_accuracy: 0.9967 - mse: 0.0026\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 971us/step - loss: 0.0094 - binary_accuracy: 0.9966 - mse: 0.0026\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0099 - binary_accuracy: 0.9963 - mse: 0.0027\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0096 - binary_accuracy: 0.9966 - mse: 0.0027\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0096 - binary_accuracy: 0.9965 - mse: 0.0027\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0095 - binary_accuracy: 0.9965 - mse: 0.0026\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0094 - binary_accuracy: 0.9965 - mse: 0.0026\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0094 - binary_accuracy: 0.9966 - mse: 0.0026\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0090 - binary_accuracy: 0.9967 - mse: 0.0025\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0090 - binary_accuracy: 0.9966 - mse: 0.0025\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0093 - binary_accuracy: 0.9965 - mse: 0.0026\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0094 - binary_accuracy: 0.9963 - mse: 0.0026\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0089 - binary_accuracy: 0.9965 - mse: 0.0025\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0096 - binary_accuracy: 0.9961 - mse: 0.0028\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0092 - binary_accuracy: 0.9964 - mse: 0.0026\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0088 - binary_accuracy: 0.9967 - mse: 0.0025\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 1ms/step - loss: 0.0086 - binary_accuracy: 0.9967 - mse: 0.0024\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 0.0088 - binary_accuracy: 0.9966 - mse: 0.0025\n",
      "Run time... 1.3481016159057617\n"
     ]
    }
   ],
   "source": [
    "model = learner(X, Y, seed=seed, hidden=hidden, optimizer=Adam(learning_rate=learning_rate))\n",
    "        \n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(X[sample], Y[sample], epochs=50, batch_size=batch_size, verbose=True)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(\"Run time...\", runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration will do for brute force runs. I'll run those with 10K iterations across several values for hidden units and see where that gets us. See brute_force_1.ipynb for the next step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
