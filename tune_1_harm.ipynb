{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-18 08:41:45.304728: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-18 08:41:45.332630: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from src.learner import *\n",
    "import time\n",
    "import pandas as pd\n",
    "from utilities import remove_cols\n",
    "\n",
    "words = pd.read_csv('data/harm/labels.csv', header=None)\n",
    "words = words[0].tolist()\n",
    "\n",
    "# inputs and outputs\n",
    "X = remove_cols(np.genfromtxt('data/harm/orth.csv', delimiter=\",\"))\n",
    "Y = remove_cols(np.genfromtxt('data/harm/phon.csv', delimiter=\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script examines whether or not we can obtain perfect predictions for the 8k (Harm) corpus by fitting individual models with specific configurations. We will do this before we move on to more sophisticated tuning. We are just trying to find a set that we can learn perfectly here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limited search across HPs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 387\n",
    "\n",
    "with open('outputs/tune_1_harm.csv', 'w') as f:\n",
    "    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                            \"hidden_units\",\n",
    "                                            \"learning_rate\",\n",
    "                                             \"batch_size\",\n",
    "                                             \"epochs\",\n",
    "                                             \"loss_train\",\n",
    "                                             \"accuracy_train\",\n",
    "                                             \"mse_train\",\n",
    "                                             \"loss_test\",\n",
    "                                             \"accuracy_test\",\n",
    "                                             \"mse_test\",\n",
    "                                             \"time\"))\n",
    "    for learning_rate in [.075, .1, .2, .25]: \n",
    "        for batch_size in [64, 128, 256]:\n",
    "            for epochs in [50, 100, 150, 250]:\n",
    "                for hidden in [100, 150, 200]:\n",
    "\n",
    "                    model = learner(X, Y, seed=seed, hidden=hidden, optimizer=Adam(learning_rate=learning_rate))    \n",
    "                    \n",
    "                    start_time = time.time()\n",
    "\n",
    "\n",
    "                    model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=False)\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    runtime = end_time - start_time\n",
    "\n",
    "                    loss_train, accuracy_train, mse_train = model.evaluate(X, Y, verbose=0) \n",
    "                    loss_test, accuracy_test, mse_test = model.evaluate(X, Y, verbose=0) \n",
    "\n",
    "                    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    learning_rate,\n",
    "                                                    batch_size,\n",
    "                                                    epochs,\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    runtime))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 387\n",
    "\n",
    "with open('outputs/tune_1_harm_2.csv', 'w') as f:\n",
    "    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                            \"hidden_units\",\n",
    "                                            \"learning_rate\",\n",
    "                                             \"batch_size\",\n",
    "                                             \"epochs\",\n",
    "                                             \"loss_train\",\n",
    "                                             \"accuracy_train\",\n",
    "                                             \"mse_train\",\n",
    "                                             \"loss_test\",\n",
    "                                             \"accuracy_test\",\n",
    "                                             \"mse_test\",\n",
    "                                             \"time\"))\n",
    "    for learning_rate in [.1, .2, .3, .4, .5, .6, .7, .8, .9]: \n",
    "        for batch_size in [256, 384, 512]:\n",
    "            for epochs in [50, 100, 150, 250, 500, 100]:\n",
    "                for hidden in [100, 150, 200]:\n",
    "\n",
    "                    model = learner(X, Y, seed=seed, hidden=hidden, optimizer=Adam(learning_rate=learning_rate))    \n",
    "                    \n",
    "                    start_time = time.time()\n",
    "\n",
    "\n",
    "                    model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose=False)\n",
    "\n",
    "                    end_time = time.time()\n",
    "                    runtime = end_time - start_time\n",
    "\n",
    "                    loss_train, accuracy_train, mse_train = model.evaluate(X, Y, verbose=0) \n",
    "                    loss_test, accuracy_test, mse_test = model.evaluate(X, Y, verbose=0) \n",
    "\n",
    "                    f.write(\"{},{},{},{},{},{},{},{},{},{},{}\\n\".format(\n",
    "                                                    hidden,\n",
    "                                                    learning_rate,\n",
    "                                                    batch_size,\n",
    "                                                    epochs,\n",
    "                                                    loss_train,\n",
    "                                                    accuracy_train,\n",
    "                                                    mse_train,\n",
    "                                                    loss_test,\n",
    "                                                    accuracy_test,\n",
    "                                                    mse_test,\n",
    "                                                    runtime))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 [==============================] - 0s 369us/step\n"
     ]
    }
   ],
   "source": [
    "accuracies = batch_accuracy(Y, model.predict(X), dichotomous=True)\n",
    "correct_words = [word for accuracy, word in zip(accuracies, words) if accuracy == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1005"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([a for a in accuracies if a == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/correct_words.csv', 'w') as f:\n",
    "    for word in correct_words:\n",
    "        f.write('{}\\n'.format(word))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of correct words were sorted to produce 900 child-appropriate words from the larger learnable set of `correct_words` above. These were written to file and read in below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a portion of the words are learned perfectly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_900 = pd.read_csv('data/harm/easy_900.csv', header=None)[0].tolist()\n",
    "sample = np.full(len(words), False, dtype=bool)\n",
    "\n",
    "indices = [i for i, e in enumerate(words) if e in easy_900]\n",
    "# Set chosen indices to True because they select the test items not the train items\n",
    "sample[indices] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4344 - binary_accuracy: 0.8149 - mse: 0.1378\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.1823 - binary_accuracy: 0.9262 - mse: 0.0523\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.1176 - binary_accuracy: 0.9605 - mse: 0.0323\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0713 - binary_accuracy: 0.9731 - mse: 0.0205\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0406 - binary_accuracy: 0.9857 - mse: 0.0111\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0266 - binary_accuracy: 0.9909 - mse: 0.0073\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0162 - binary_accuracy: 0.9954 - mse: 0.0040\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0113 - binary_accuracy: 0.9973 - mse: 0.0026\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0074 - binary_accuracy: 0.9984 - mse: 0.0015\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0055 - binary_accuracy: 0.9988 - mse: 0.0011\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0039 - binary_accuracy: 0.9994 - mse: 6.9607e-04\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0031 - binary_accuracy: 0.9996 - mse: 4.9069e-04\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0024 - binary_accuracy: 0.9997 - mse: 3.5717e-04\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0020 - binary_accuracy: 0.9998 - mse: 2.4884e-04\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0017 - binary_accuracy: 0.9999 - mse: 1.8840e-04\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0015 - binary_accuracy: 0.9999 - mse: 1.5357e-04\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0013 - binary_accuracy: 0.9999 - mse: 1.2936e-04\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.0012 - binary_accuracy: 0.9999 - mse: 1.1534e-04\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0010 - binary_accuracy: 0.9999 - mse: 1.0305e-04\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 9.5856e-04 - binary_accuracy: 0.9999 - mse: 8.8565e-05\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 8.9354e-04 - binary_accuracy: 0.9999 - mse: 7.8837e-05\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 8.3528e-04 - binary_accuracy: 0.9999 - mse: 7.1764e-05\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.8069e-04 - binary_accuracy: 1.0000 - mse: 6.5396e-05\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.4420e-04 - binary_accuracy: 0.9999 - mse: 6.5441e-05\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 7.0640e-04 - binary_accuracy: 0.9999 - mse: 6.2293e-05\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.6654e-04 - binary_accuracy: 1.0000 - mse: 5.5385e-05\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 6.3593e-04 - binary_accuracy: 1.0000 - mse: 5.5204e-05\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 6.0012e-04 - binary_accuracy: 1.0000 - mse: 5.1015e-05\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.8429e-04 - binary_accuracy: 1.0000 - mse: 4.9657e-05\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.5832e-04 - binary_accuracy: 1.0000 - mse: 4.6239e-05\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.3061e-04 - binary_accuracy: 1.0000 - mse: 4.3089e-05\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 5.1011e-04 - binary_accuracy: 1.0000 - mse: 4.1433e-05\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.8669e-04 - binary_accuracy: 1.0000 - mse: 3.9075e-05\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.6814e-04 - binary_accuracy: 1.0000 - mse: 3.8386e-05\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.5487e-04 - binary_accuracy: 1.0000 - mse: 3.7950e-05\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.3765e-04 - binary_accuracy: 1.0000 - mse: 3.6048e-05\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 4.2682e-04 - binary_accuracy: 1.0000 - mse: 3.4229e-05\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 4.1123e-04 - binary_accuracy: 1.0000 - mse: 3.1371e-05\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8984e-04 - binary_accuracy: 1.0000 - mse: 2.9874e-05\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.8035e-04 - binary_accuracy: 1.0000 - mse: 3.1096e-05\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.7078e-04 - binary_accuracy: 1.0000 - mse: 2.9988e-05\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 3.5388e-04 - binary_accuracy: 1.0000 - mse: 2.6524e-05\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.4331e-04 - binary_accuracy: 1.0000 - mse: 2.6693e-05\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 3.3030e-04 - binary_accuracy: 1.0000 - mse: 2.5582e-05\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.1851e-04 - binary_accuracy: 1.0000 - mse: 2.4734e-05\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0935e-04 - binary_accuracy: 1.0000 - mse: 2.3700e-05\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3.0433e-04 - binary_accuracy: 1.0000 - mse: 2.3794e-05\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 2.9316e-04 - binary_accuracy: 1.0000 - mse: 2.2648e-05\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8612e-04 - binary_accuracy: 1.0000 - mse: 2.0940e-05\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 2.8190e-04 - binary_accuracy: 1.0000 - mse: 2.0549e-05\n"
     ]
    }
   ],
   "source": [
    "model = learner(X, Y, seed=seed, hidden=100, optimizer=Adam(learning_rate=.075))    \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model.fit(X[sample], Y[sample], epochs=50, batch_size=256, verbose=True)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so the easy 900 set can be learned perfectly and quickly. Let's move on and run simulations with these at least for now. Let's see what we can find in the hyperparameter space first though. That can be found in `tune_900...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
